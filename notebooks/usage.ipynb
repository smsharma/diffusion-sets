{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "\n",
    "from flax.core import FrozenDict\n",
    "\n",
    "from models.diffusion import VariationalDiffusionModel\n",
    "from models.diffusion_utils import generate, loss_vdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer args\n",
    "transformer_dict = FrozenDict({\"d_model\":256, \"d_mlp\":512, \"n_layers\":5, \"n_heads\":4, \"flash_attention\":True})\n",
    "\n",
    "# Instantiate model\n",
    "vdm = VariationalDiffusionModel(gamma_min=-6.0, gamma_max=6.0,  # Noise schedule parameters\n",
    "          d_feature=4,  # Number of features per set element\n",
    "          transformer_dict=transformer_dict,  # Score-prediction transformer parameters\n",
    "          noise_schedule=\"learned_linear\",  # Noise schedule; \"learned_linear\" or \"scalar\"\n",
    "          n_layers=3,  # Layers in encoder/decoder element-wise ResNets\n",
    "          d_embedding=8,  # Dim to encode the per-element features to\n",
    "          d_hidden_encoding=64,  # Hidden dim used in various contexts (for embedding context, 4 * for encoding/decoding in ResNets)\n",
    "          timesteps=300,  # Number of diffusion steps\n",
    "          d_t_embedding=16,  # Timestep embedding dimension\n",
    "          noise_scale=1e-3,  # Data noise model\n",
    "          n_classes=0)  # Number of data classes. If >0, the first element of the conditioning vector is assumed to be the integer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "x = jax.random.normal(rng, (32, 100, 4))\n",
    "mask = jax.random.randint(rng, (32, 100), 0, 2)\n",
    "conditioning = jax.random.normal(rng, (32, 6))\n",
    "\n",
    "# Call to get losses\n",
    "(loss_diff, loss_klz, loss_recon), params = vdm.init_with_output({\"sample\": rng, \"params\": rng, \"uncond\":rng}, x, conditioning, mask);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(5606182.5, dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute full loss, accounting for masking\n",
    "loss_vdm(params, vdm, rng, x, conditioning, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from model\n",
    "\n",
    "mask_sample = jax.random.randint(rng, (24, 100), 0, 2)\n",
    "conditionink_sample = jax.random.normal(rng, (24, 6))\n",
    "\n",
    "x_samples = generate(vdm, params, rng, (24, 100), conditionink_sample, mask_sample)\n",
    "x_samples.mean().shape  # Mean of decoded Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
