{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "# ! pip install flax optax jetnet\n",
    "# ! pip install flash_attention_jax\n",
    "# ! pip install tensorflow-probability\n",
    "# ! pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "import optax\n",
    "import flax\n",
    "from flax.core import FrozenDict\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Ensure TF does not see GPU and grab all GPU memory\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "replicate = flax.jax_utils.replicate\n",
    "unreplicate = flax.jax_utils.unreplicate\n",
    "\n",
    "from models.diffusion import VariationalDiffusionModel\n",
    "from models.diffusion_utils import loss_vdm, sigma2, generate\n",
    "from models.train_utils import create_input_iter, param_count, StateStore, train_step\n",
    "\n",
    "from jetnet.datasets import JetNet, TopTagging\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export XLA_FLAGS=--xla_gpu_force_compilation_parallelism=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_force_compilation_parallelism=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"/n/dvorkin_lab/smsharma/functional-diffusion/notebooks/ckpts/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles = 5000\n",
    "n_features = 3\n",
    "\n",
    "x = np.load(\"/n/holyscratch01/iaifi_lab/ccuesta/data_for_sid/halos.npy\")\n",
    "x_mean = x.mean(axis=(0,))\n",
    "x_std = x.std(axis=(0,))\n",
    "x = (x - x_mean + 1e-7) / (x_std + 1e-7)\n",
    "\n",
    "x = x[:, :n_particles, :n_features]\n",
    "x = np.pad(x, [(0, 0), (0, 5120 - n_particles), (0, 0)])\n",
    "conditioning = np.array(pd.read_csv(\"/n/holyscratch01/iaifi_lab/ccuesta/data_for_sid/cosmology.csv\").values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones((x.shape[0], n_particles))\n",
    "mask = np.pad(mask, [(0, 0), (0, 5120 - n_particles)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 * jax.device_count()\n",
    "n_train = len(x)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x, conditioning, mask))\n",
    "train_ds = train_ds.cache()\n",
    "train_ds = train_ds.repeat()\n",
    "\n",
    "batch_dims=[jax.local_device_count(), batch_size // jax.device_count()]\n",
    "\n",
    "for batch_size in reversed(batch_dims):\n",
    "      train_ds = train_ds.batch(batch_size, drop_remainder=False)\n",
    "\n",
    "train_ds = train_ds.shuffle(n_train, seed=42)\n",
    "train_df = create_input_iter(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_dict = FrozenDict({\"d_model\":256, \"d_mlp\":512, \"n_layers\":5, \"n_heads\":4, \"flash_attention\":True})  # Transformer args\n",
    "\n",
    "vdm = VariationalDiffusionModel(gamma_min=-6.0, gamma_max=6.0, \n",
    "          n_layers=3, \n",
    "          d_embedding=8,\n",
    "          d_hidden_encoding=32,\n",
    "          timesteps=300, \n",
    "          d_t_embedding=16,\n",
    "          d_feature=n_features,\n",
    "          latent_diffusion=True,\n",
    "          transformer_dict=transformer_dict,\n",
    "          n_classes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = create_input_iter(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Past a test batch through to initialize model\n",
    "\n",
    "x_batch, conditioning_batch, mask_batch = next(batches)\n",
    "rng = jax.random.PRNGKey(42)\n",
    "out, params = vdm.init_with_output({\"sample\":rng, \"params\":rng, \"uncond\":rng}, x_batch[0], conditioning_batch[0], mask_batch[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 3,550,043\n"
     ]
    }
   ],
   "source": [
    "print(f\"Params: {param_count(params):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(3613346.5, dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the loss for good measure\n",
    "loss_vdm(params, vdm, rng, x_batch[0], conditioning_batch[0], mask_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 50_000 // jax.device_count()\n",
    "save_every = 5000 // jax.device_count()\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.scale_by_schedule(optax.cosine_decay_schedule(1.0, train_steps, 1e-5)),\n",
    "    optax.adamw(3e-4, weight_decay=1e-4),\n",
    "    optax.scale_by_schedule(optax.linear_schedule(0.0, 1.0, 5000)))\n",
    "\n",
    "store = StateStore(params, opt.init(params), rng, 0)\n",
    "pstore = replicate(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 1344/12500 [10:33<1:24:56,  2.19it/s, val=2147.375] "
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from flax.training import checkpoints\n",
    "\n",
    "vals = []\n",
    "with trange(train_steps) as t:\n",
    "    for i in t:\n",
    "        pstore, val = train_step(pstore, loss_vdm, vdm, next(batches), opt)\n",
    "        v = unreplicate(val)\n",
    "        t.set_postfix(val=v)\n",
    "        vals.append(v)\n",
    "\n",
    "        if i % save_every == 0:\n",
    "            ckpt = unreplicate(pstore)\n",
    "            checkpoints.save_checkpoint(ckpt_dir=ckpt_dir, target=ckpt, step=i, overwrite=True, keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_state = StateStore(params, opt.init(params), rng, 0)\n",
    "restored_state = checkpoints.restore_checkpoint(ckpt_dir=ckpt_dir, target=empty_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
